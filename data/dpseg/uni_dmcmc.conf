# ngram = 1
# a1 = 0
# b1 = 20

# #uncomment these and comment above, if running bigrams
# ngram = 2
# b1 = 3000
# a2 = 0
# b2 = 100

eval-maximize = 1

# if using online learners (Decayed MCMC, Tree Sampler, Viterbi), this
# will be after how many utterances rather than how many iterations
eval-interval = 500

estimator = D
mode = online
decay-rate = 1.5
samples-per-utt = 20000

# if no annealing, set this to 0; otherwise = burnin-iterations
# (though if annealing start and stop temperatures are both 1 below,
# then shouldn't do anything anyway)
anneal-iterations = 0
# burn-in is total iterations through training data
burnin-iterations = 1
trace-every = 2000
debug-level = 100

# probably don't change the stuff below
hypersamp-ratio = 0
nchartypes = 0
init-pboundary = 0.5
# originally 10 for annealing; set to 1 if no annealing
anneal-start-temperature = 1
anneal-stop-temperature = 1
anneal-a = 10

randseed=12780258
