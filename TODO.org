* TODO for wordseg-0.6 [0/8]
- [ ] implement wordseg-syll
  adaptation of new-syllabify-corpus.pl
- [ ] debug the type/token evaluation
  which are now the same.
- [ ] debug dibs
  must output the same results as the original one from R. Dala
- [ ] implement a wordseg-cha (or a separate cha2clean package?)
  from the cha2sel and sel2clean scripts
- [ ] add the "corpus description" and entropy in wordseg-stats
- [ ] implement wordseg-baseline
- [ ] debug wordseg-ag so that it can compile painlessly on MacOS
- [ ] optimize wordseg-tp (slower than before according to Georgia)
* TODO for a next release [0/7]
- [ ] add functions in the statistics module
- [ ] refactor segment() in ag and dpseg
  We want an easy access to arguments from Python and from config file
- [ ] optimize dpseg and ag
- [ ] refactor C++ code in ag
  C++11, stdlib, etc.
- [ ] make a wordseg.so for dpseg/ag common code
- [ ] write a ChangeLog.md
- [ ] refactor the html documentation
  less pages for general, more pages for API, Python docstrings in website

* Mail from Alex
I know you're working on the things we discussed yesterday (since some
phonemization for rare languages will be done by rewrite rules, adding
back the syllabification), but I realized there were a few more things
that fell out from the package and/or might be useful to add, as
follows.

Looking through the instructions, I realize the "selection and
cleaning" stage is gone -- i.e., if someone starts with .cha files
they'll have to re-implement the selection and cleaning that we have
in the CDSwordseg. I agree that this is conceptually separate and
would be different for every input style, so perhaps we should have a
mini-package that JUST does the cleaning for the CHA files. I wouldn't
want for this knowledge to disappear forever because I think many in
the community are re-inventing this wheel, and sometimes not
perfectly... (I know we had loads of bugs there...)

It would be very easy to implement a "corpus description" tool like
this one
(https://github.com/alecristia/CDSwordSeg/blob/master/recipes/CatalanSpanish/_describe_gold.sh). Do
you think you can add it? If you think things should be only in
python, it looks like this script
(https://github.com/ConstantineLignos/WordSegmentation/blob/master/tools/corpus_statistics.py)
from a fellow segmenter has many things we have too. (Although I'd
rather we stuck to our code -- other than altering where output such
as the list of words and syllables is written --, since I know what it
does...)

One thing that is missing from our gold description script is the
"entropy" measurement Emmanuel and Abdellah Fourtassi had implemented
in this paper (http://aclweb.org/anthology/W/W13/W13-2601.pdf): We are
interested in the ambiguity generated by the different possible parses
that result from such a supervised segmentation. In order to quantify
this idea in general, we define a Normalized Segmentation Entropy. To
do this, we need to assign a probability to every possible
segmentation. To this end, we use a unigram model where the
probability of a lexical item is its normalized frequency in the
corpus and the probability of a parse is the product of the
probabilities of its terms. In order to obtain a measure that does not
depend on the utterance length, we normalize by the number of possible
boundaries in the utterance. So for an utterance of length N, the
Normalized Segmentation Entropy (NSE) is computed using Shannon
formula (Shannon, 1948) as follows: — NSE = − !  i Pilog2(Pi)/(N − 1)
— where Pi is the probability of the parse i .

Finally, I wanted to mention one tool we never had in CDSwordseg and
might not be urgent to implement, but could be useful: A baseline
random segmentation algorithm, to see what is the baseline level of
performance in a given language/corpus. This algorithm would get a
corpus and a parameter which is the likelihood of a break, and just
applies it randomly: An alternative baseline explored by Lignos (2012)
is a random oracle segmenter (RandOracle). The random oracle segments
a corpus by treating each possible boundary location as a Bernoulli
trial. The segmenter is an “oracle” in that it knows the true
probability of a boundary in the corpus. The random oracle inserts
boundaries with this same probability. This acts as a baseline for
comparison, representing the performance that could be achieved by
randomly guessing at the true boundary rate. Any model which does not
surpass the RandOracle model should have low enough performance that
it could be ruled out as a possible model of early infant
segmentation..

So to sum up:
- we definitely want to add back the selection+cleaning code from CHAT
  files, (cha2sel, sel2clean)
- we might want to add back tools for corpus description (including
  entropy); and a baseline segmentor.
